# Project 3: Generative Visual

Owen Jow, owen@eng.ucsd.edu

## Abstract

I create a 3D scene, along with a corresponding top-down stylization map that describes how each part of the scene should be stylized. The map is smooth. Then, I allow users to interactively walk around the scene, with the final rendering for each viewpoint determined as a stylized version of a fast OpenGL render. The idea is that some parts of the scene are intentionally stylized to be "more beautiful" than others, and the user can, if he/she chooses, search for this beauty. Also, in keeping with the theme from my past projects, I include a [chicken](https://www.turbosquid.com/3d-models/christmas-chicken-grey-art-3d-1266316) in the scene.

I use a real-time implementation of CycleGAN.

(alternative idea: 3D Lego mapping)

## Model/Data

Briefly describe the files that are included with your repository:
- trained models
- training data (or link to training data)

## Code

Your code for generating your project:
- Python: generative_code.py
- Jupyter notebooks: generative_code.ipynb

## Results

Documentation of your results in an appropriate format, both links to files and a brief description of their contents:
- image files (`.jpg`, '.png' or whatever else is appropriate)
- movie files (uploaded to youtube or vimeo due to github file size limits)
- ... some other form

## Technical Notes

Any implementation details or notes we need to repeat your work. 
- Does this code require other pip packages, software, etc?
- Does it run on some other (non-datahub) platform? (CoLab, etc.)

## References

References to any papers, techniques, repositories you used:
- Papers
- Repositories
- Blog posts
